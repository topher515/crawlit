Crawlit is a redis-backed, Docker-containerized, parallelized web crawler. 

Each instance of a Crawlit container (when linked to the same Redis container)
can work in parallel to perform crawling. As additional Crawlit containers
are instantiated they will process crawls in parallel.

## Starting Crawlit 

    # Build the crawlit container
    docker build -t topher515/crawlit:latest .

    # Create a working redis container
    docker run --name=short_mocha redis

    # Run the crawlit container linked to your redis container
    # (Specify a known port for at least one instance of crawlit)
    docker run --link=short_mocha:redis -p 0.0.0.0:49155:80 -t -i topher515/crawlit:latest

    # (Successive containers can have any random port)
    docker run --link=short_mocha:redis -P -t -i topher515/crawlit:latest

## Starting crawls

    # POST a `\n` seperated list of URLs to enqueue URLs for crawling
    curl -X POST -d@- http://localhost:49155/ << EOF
    http://www.docker.com/
    http://www.cnn.com/
    http://www.4chan.com/
    EOF

    # Note that on Macs w/ `boot2docker` you need to specify the host this way
    curl -X POST -d@- http://$(/usr/local/bin/boot2docker ip 2>/dev/null):49155/ << EOF
    http://www.docker.com/
    http://www.cnn.com/
    http://www.4chan.com/
    EOF

If the job starts properly you will receive a 200 response and a `job_id` 


## Considerations when building `Crawlit`

### Assumed Requirements
- We don't care about web content generated by Javascript
- `job_id`s are unique forever; always refer to the same job
- `job_id`s constently reference the same jobs across container instances
- We only care about `<a href=""></a>` formatted links.
- We don't care about job status/result privacy
- We don't care about rate-limiting inputs

### Minor assumptions
- The list of input URLs can fit into container memory

### Known Bugs
- Sometimes SSL requests break for unknown reasons
- the # of `inprogress` crawls is sometimes `n+1` where `n` is the true number of crawls







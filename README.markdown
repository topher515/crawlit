

### Using Crawlit 

# Build the crawlit container
docker build -t topher515/crawlit:latest .

# Create a working redis container
docker run --name=shivering_wombat redis

# Make sure redis blah blah:
REDIS> config set notify-keyspace-events "Kl"


# Run the crawlit instances linked to your redis container
# (Specify a known port for at least one instance of crawlit)
docker run --link=shivering_wombat:redis -p 0.0.0.0:49155:80 -t -i topher515/crawlit:latest

# (Successive containers can have any random port)
docker run --link=shivering_wombat:redis -P -t -i topher515/crawlit:latest



# POST a `\n` seperated list of URLs to enqueue URLs for crawling
curl -X POST -d@- http://localhost:49155/ << EOF
http://www.docker.com/
http://www.cnn.com/
http://www.4chan.com/
EOF


# Note that on Macs w/ `boot2docker` you need to specify the host this way
curl -X POST -d@- http://$(/usr/local/bin/boot2docker ip 2>/dev/null):49155/ << EOF
http://www.docker.com/
http://www.cnn.com/
http://www.4chan.com/
EOF



### Considerations when building `Crawlit`

## Assumed Requirements
- We don't care about web content generated by Javascript
- `job_id`s are unique forever; always refer to the same job
- `job_id`s constently reference the same jobs across container instances
- We only care about `<a href=""></a>` formatted links.
- We don't care about job status/result privacy
- We don't care about rate-limiting inputs

## Minor assumptions
- The list of input URLs can fit into container memory

## Known Bugs
- Sometimes SSL requests break for unknown reasons
- the # of `inprogress` crawls is sometimes `n+1` where `n` is the true number of crawls






